"""
SemiProcess MCP Server - ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ë¶„ì„ ë„êµ¬ ì§‘í•© (15ê°œ Tool)
"""

from typing import Any, Dict, List, Optional
import statistics

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, Response

app = FastAPI(title="SemiProcess MCP Server")

DISCLAIMER = """
> ğŸ“Œ **ë¶„ì„ ê¸°ì¤€ ì•ˆë‚´**
>
> ë³¸ ë¶„ì„ì€ ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤:
> - **ì…ë ¥ ë°ì´í„°**: ì‚¬ìš©ì ì œê³µ (ì •í™•ì„±ì€ ì‚¬ìš©ì ì±…ì„)
> - **ê³„ì‚° ë¡œì§**: ì‚°ì—… í‘œì¤€ ë°©ë²•ë¡ ì„ ì°¸ê³ (ISO/AIAG/FMEA/DOE ë“±)í•˜ë˜, ë‹¨ìˆœí™”ëœ ê³„ì‚°ì„ ì ìš©
> - **ê²°ê³¼ í™œìš©**: ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì˜ì‚¬ê²°ì • ì „ ì „ë¬¸ê°€ ê²€í†  ê¶Œì¥
>
> âš ï¸ ì‹¤ë¬´ ì ìš© ì‹œ ì‚¬ë‚´ í‘œì¤€, ì‹¤ì¸¡ ë°ì´í„°, ì „ë¬¸ê°€ ê²€ì¦ì„ ë°˜ë“œì‹œ ë³‘í–‰í•˜ì„¸ìš”.
"""


def _missing(required: List[str], provided: Dict[str, Any]) -> List[str]:
    return [k for k in required if provided.get(k) is None]


def _err_missing(missing: List[str]) -> str:
    items = "\n".join(f"- `{m}`" for m in missing)
    return f"{DISCLAIMER}\n\n## âš ï¸ ì…ë ¥ ì˜¤ë¥˜\ní•„ìˆ˜ ì…ë ¥ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\n{items}"


# CSV íŒŒì‹± í—¬í¼ í•¨ìˆ˜ë“¤
def _parse_csv_records(records_csv: str) -> List[Dict[str, Any]]:
    """CSV í˜•ì‹: 'ë‚ ì§œ,ì¥ë¹„,ìˆ˜ëŸ‰,ì¡°ì¹˜,ê²°ê³¼' ê° í–‰ì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„"""
    records = []
    for row in records_csv.split(';'):
        row = row.strip()
        if not row:
            continue
        parts = [p.strip() for p in row.split(',')]
        if len(parts) >= 5:
            records.append({
                'date': parts[0],
                'equipment_id': parts[1],
                'wafer_count': int(parts[2]) if parts[2].isdigit() else 0,
                'action_taken': parts[3],
                'result': parts[4],
                'defect_type': 'SCRATCH'
            })
    return records


def _parse_csv_dict(csv_str: str, separator: str = ',') -> Dict[str, Any]:
    """CSV í˜•ì‹: 'key:value' ì‰¼í‘œë¡œ êµ¬ë¶„. % ê¸°í˜¸ ì œê±° í›„ float ë³€í™˜"""
    result = {}
    for item in csv_str.split(separator):
        item = item.strip()
        if ':' in item:
            key, value = item.split(':', 1)
            key = key.strip()
            value = value.strip().rstrip('%')  # % ê¸°í˜¸ ì œê±°
            try:
                result[key] = float(value) if '.' in value else int(value)
            except (ValueError, TypeError):
                result[key] = value
    return result


def _parse_window_params(window_csv: str) -> Dict[str, Dict[str, float]]:
    """íŒŒë¼ë¯¸í„°:ìµœì†Œ:ìµœëŒ€ ë˜ëŠ” íŒŒë¼ë¯¸í„°:ìµœì†Œ-ìµœëŒ€ í˜•ì‹"""
    result = {}
    for item in window_csv.split(','):
        item = item.strip()
        if ':' in item:
            parts = [p.strip() for p in item.split(':')]
            if len(parts) >= 2:
                param = parts[0]
                # ë‘ ë²ˆì§¸ ë¶€ë¶„ì—ì„œ ëŒ€ì‹œ(min-max) ë˜ëŠ” ì½œë¡ (min:max) ë¶„ë¦¬
                range_str = ':'.join(parts[1:])
                if '-' in range_str and ':' not in range_str[range_str.index('-')+1:]:
                    # ëŒ€ì‹œ í¬ë§·: "450-500"
                    min_max = [p.strip() for p in range_str.split('-')]
                    if len(min_max) == 2:
                        try:
                            result[param] = {'min': float(min_max[0]), 'max': float(min_max[1])}
                        except (ValueError, TypeError):
                            pass
                elif len(parts) >= 3:
                    # ì½œë¡  í¬ë§·: "450:500"
                    try:
                        result[param] = {'min': float(parts[1]), 'max': float(parts[2])}
                    except (ValueError, TypeError):
                        pass
    return result


def _parse_baseline_params(baseline_csv: str) -> Dict[str, Dict[str, Any]]:
    """íŒŒë¼ë¯¸í„°:í‘œì¤€ê°’:ìµœì†Œ:ìµœëŒ€:ë‹¨ìœ„ í˜•ì‹"""
    result = {}
    for item in baseline_csv.split(','):
        item = item.strip()
        if ':' in item:
            parts = [p.strip() for p in item.split(':')]
            if len(parts) >= 3:
                try:
                    result[parts[0]] = {
                        'value': float(parts[1]),
                        'min': float(parts[2]),
                        'max': float(parts[3]),
                        'unit': parts[4] if len(parts) > 4 else ''
                    }
                except (ValueError, TypeError):
                    pass
    return result


def _parse_recipe_params(recipe_csv: str) -> Dict[str, float]:
    """íŒŒë¼ë¯¸í„°:ê°’ í˜•ì‹"""
    result = {}
    for item in recipe_csv.split(','):
        item = item.strip()
        if ':' in item:
            key, value = item.split(':', 1)
            key = key.strip()
            value = value.strip()
            try:
                result[key] = float(value)
            except (ValueError, TypeError):
                result[key] = value
    return result


def analyze_defect(
    defect_code: str,
    defect_description: str,
    process_step: str,
    equipment_id: str = None,
    wafer_id: str = None,
    known_causes: Optional[List[str]] = None,
    recent_changes: Optional[List[str]] = None,
) -> str:
    miss = _missing(["defect_code", "defect_description", "process_step"], locals())
    if miss:
        return _err_missing(miss)
    causes = "\n".join(f"- {c}" for c in (known_causes or [])) or "- (ì‚¬ìš©ì ì›ì¸ ë¯¸ì…ë ¥)"
    changes = "\n".join(f"- {c}" for c in (recent_changes or [])) or "- ìµœê·¼ ë³€ê²½ ì—†ìŒ ë³´ê³ "
    return (
        f"{DISCLAIMER}\n\n## ğŸ” ë¶ˆëŸ‰ ë¶„ì„\n"
        f"- ì½”ë“œ: {defect_code}\n- ì„¤ëª…: {defect_description}\n- ê³µì •: {process_step}\n"
        f"- ì¥ë¹„: {equipment_id or 'ë¯¸ì…ë ¥'} / ì›¨ì´í¼: {wafer_id or 'ë¯¸ì…ë ¥'}\n\n"
        f"### ì‚¬ìš©ì ì œì•ˆ ì›ì¸\n{causes}\n\n"
        f"### ì¼ë°˜ ì ê²€\n- ì¥ë¹„ ì•ŒëŒ/ë¡œê·¸\n- ìµœê·¼ PM/ìº˜ë¦¬ë¸Œë ˆì´ì…˜\n- ë ˆì‹œí”¼ ë³€ê²½ ì´ë ¥\n- ì†Œì¬/ì¼€ë¯¸ Lot\n- SPC/Lot í¸ì°¨\n\n"
        f"### ìµœê·¼ ë³€ê²½ ì‚¬í•­\n{changes}\n"
    )


def get_defect_history(defect_type: str = None, records_csv: str = None, defect_records: List[Dict[str, Any]] = None, analysis_type: str = "trend") -> str:
    # CSV í˜•ì‹ ì§€ì› (CSV ìš°ì„ )
    if records_csv and not defect_records:
        defect_records = _parse_csv_records(records_csv)
    
    miss = _missing(["defect_records"], {"defect_records": defect_records})
    if miss:
        return _err_missing(miss)
    if not defect_records:
        return f"{DISCLAIMER}\n\n## âš ï¸ ì…ë ¥ ì˜¤ë¥˜\në¶ˆëŸ‰ ì´ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
    total = len(defect_records)
    wafer_sum = sum(r.get("wafer_count", 0) for r in defect_records)
    actions = [r.get("action_taken", "") for r in defect_records if r.get("action_taken")]
    unique_actions = list({a for a in actions})
    equip_counter: Dict[str, int] = {}
    for r in defect_records:
        eq = r.get("equipment_id", "ë¯¸ì…ë ¥")
        equip_counter[eq] = equip_counter.get(eq, 0) + 1
    top_equipment = sorted(equip_counter.items(), key=lambda x: x[1], reverse=True)
    top_equipment_text = "\n".join([f"- {eq}: {cnt}íšŒ" for eq, cnt in top_equipment[:3]]) or "- ë°ì´í„° ë¶€ì¡±"
    rows = "\n".join(
        f"| {r.get('date','-')} | {r.get('defect_type','-')} | {r.get('equipment_id','-')} | {r.get('wafer_count','-')} | {r.get('action_taken','-')} | {r.get('result','-')} |"
        for r in defect_records
    )
    return (
        f"{DISCLAIMER}\n\n## ğŸ“Š ë¶ˆëŸ‰ ì´ë ¥ ë¶„ì„ ({analysis_type})\n"
        f"- ì´ ì´ë ¥: {total}ê±´\n- ë¶ˆëŸ‰ ì›¨ì´í¼ í•©ê³„: {wafer_sum}ë§¤\n"
        f"- ì‚¬ìš©ëœ ì¡°ì¹˜: {', '.join(unique_actions) if unique_actions else 'ì¡°ì¹˜ ì •ë³´ ë¶€ì¡±'}\n\n"
        f"| ë‚ ì§œ | ë¶ˆëŸ‰ | ì¥ë¹„ | ì›¨ì´í¼ | ì¡°ì¹˜ | ê²°ê³¼ |\n|------|------|------|--------|------|------|\n{rows}\n\n"
        f"### íŒ¨í„´ ë°œê²¬\n- ì¥ë¹„ ì§‘ì¤‘ë„ ìƒìœ„\n{top_equipment_text}\n"
    )


def suggest_corrective_action(
    problem_description: str,
    affected_equipment: str,
    severity: str,
    current_status: str,
    available_resources: Optional[List[str]] = None,
    time_constraint: str = None,
) -> str:
    miss = _missing(["problem_description", "affected_equipment", "severity", "current_status"], locals())
    if miss:
        return _err_missing(miss)
    sev = severity.lower()
    immediate = {
        "critical": ["ì¦‰ì‹œ ì¥ë¹„ ì •ì§€", "ì˜í–¥ Lot ê²©ë¦¬", "ì „ë¬¸ ì—”ì§€ë‹ˆì–´ í˜¸ì¶œ"],
        "major": ["ê³µì • ì¼ì‹œ ì¤‘ì§€", "ì¡°ê±´ ì ê²€", "ì•ŒëŒ/ë¡œê·¸ ìˆ˜ì§‘"],
        "minor": ["ì¡°ê±´ ë¯¸ì„¸ ì¡°ì •", "ëª¨ë‹ˆí„°ë§ ê°•í™”"],
    }.get(sev, ["ìƒí™© í‰ê°€ í›„ ê²°ì •"])
    resources = "\n".join(f"- {r}" for r in (available_resources or ["ìì› ë¯¸ì…ë ¥"]))
    return (
        f"{DISCLAIMER}\n\n## ğŸ”§ ì‹œì • ì¡°ì¹˜ ì œì•ˆ\n"
        f"- ë¬¸ì œ: {problem_description}\n- ì¥ë¹„: {affected_equipment}\n- ì‹¬ê°ë„: {severity}\n- ìƒíƒœ: {current_status}\n- ì‹œê°„ ì œì•½: {time_constraint or 'ë¯¸ì…ë ¥'}\n\n"
        f"### ì¦‰ì‹œ ì¡°ì¹˜\n" + "\n".join(f"{i+1}. {v}" for i, v in enumerate(immediate)) + "\n\n"
        f"### í•„ìš” ìì›\n{resources}\n"
    )


def compare_to_baseline(
    recipe_name: str = None,
    baseline_params: str = None,
    current_params: str = None,
    baseline_recipe: Dict[str, Dict[str, Any]] = None,
    current_recipe: Dict[str, float] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if baseline_params and not baseline_recipe:
        baseline_recipe = _parse_baseline_params(baseline_params)
    if current_params and not current_recipe:
        current_recipe = _parse_recipe_params(current_params)
    
    miss = _missing(["baseline_recipe", "current_recipe"], {"baseline_recipe": baseline_recipe, "current_recipe": current_recipe})
    if miss:
        return _err_missing(miss)
    rows = []
    for p, meta in baseline_recipe.items():
        cur = current_recipe.get(p)
        status = "âœ…"
        min_v, max_v = meta.get("min"), meta.get("max")
        if cur is None:
            status = "âš ï¸ ë¯¸ì…ë ¥"
        elif (min_v is not None and cur < min_v) or (max_v is not None and cur > max_v):
            status = "âŒ ì´íƒˆ"
        rows.append(f"| {p} | {meta.get('value')} {meta.get('unit','')} | {cur} | {status} |")
    table = "\n".join(rows) if rows else "| - | - | - | - |"
    return (
        f"{DISCLAIMER}\n\n## ğŸ“ ê¸°ì¤€ ëŒ€ë¹„ ë¹„êµ\n- ë ˆì‹œí”¼: {recipe_name or 'ë¯¸ì…ë ¥'}\n\n"
        f"| íŒŒë¼ë¯¸í„° | ê¸°ì¤€ | í˜„ì¬ | ìƒíƒœ |\n|----------|------|------|------|\n{table}\n"
    )


def compare_two_recipes(
    recipe_a_name: str = None,
    recipe_a_params: str = None,
    recipe_b_name: str = None,
    recipe_b_params: str = None,
    tolerance_params: str = None,
    recipe_a: Dict[str, float] = None,
    recipe_b: Dict[str, float] = None,
    tolerance: Optional[Dict[str, float]] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if recipe_a_params and not recipe_a:
        recipe_a = _parse_recipe_params(recipe_a_params)
    if recipe_b_params and not recipe_b:
        recipe_b = _parse_recipe_params(recipe_b_params)
    if tolerance_params and not tolerance:
        tolerance = _parse_csv_dict(tolerance_params, ',')
    
    miss = _missing(["recipe_a", "recipe_b"], {"recipe_a": recipe_a, "recipe_b": recipe_b})
    if miss:
        return _err_missing(miss)
    rows = []
    all_params = set(recipe_a.keys()) | set(recipe_b.keys())
    for p in sorted(all_params):
        a, b = recipe_a.get(p), recipe_b.get(p)
        status = "âœ…"
        if tolerance and p in tolerance and a is not None and b is not None:
            diff_pct = ((b - a) / a * 100) if a else 0
            if abs(diff_pct) > tolerance[p]:
                status = "âŒ ì´ˆê³¼"
        rows.append(f"| {p} | {a} | {b} | {status} |")
    table = "\n".join(rows)
    return (
        f"{DISCLAIMER}\n\n## ğŸ”„ ë‘ ë ˆì‹œí”¼ ë¹„êµ\n- {recipe_a_name or 'Recipe A'} vs {recipe_b_name or 'Recipe B'}\n\n"
        f"| íŒŒë¼ë¯¸í„° | {recipe_a_name or 'Recipe A'} | {recipe_b_name or 'Recipe B'} | ìƒíƒœ |\n|----------|---------------|---------------|------|\n{table}\n"
    )


def validate_process_window(
    process_name: str = None,
    window_params: str = None,
    test_params: str = None,
    critical_params: str = None,
    process_window: Dict[str, Dict[str, Any]] = None,
    test_conditions: Dict[str, float] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if window_params and not process_window:
        process_window = _parse_window_params(window_params)
    if test_params and not test_conditions:
        test_conditions = _parse_recipe_params(test_params)
    
    critical_list = []
    if isinstance(critical_params, str):
        critical_list = [p.strip() for p in critical_params.split(',')]
    elif isinstance(critical_params, list):
        critical_list = critical_params
    
    miss = _missing(["process_window", "test_conditions"], {"process_window": process_window, "test_conditions": test_conditions})
    if miss:
        return _err_missing(miss)
    rows = []
    alerts = []
    for p, lim in process_window.items():
        val = test_conditions.get(p)
        min_v, max_v = lim.get("min"), lim.get("max")
        status = "âœ… PASS"
        if val is None or (min_v is not None and val < min_v) or (max_v is not None and val > max_v):
            status = "âŒ FAIL"
            if critical_list and p in critical_list:
                alerts.append(f"- ì¤‘ìš” {p}: {val} (ë²”ìœ„ {min_v}-{max_v})")
        rows.append(f"| {p} | {val} | {min_v}-{max_v} | {status} |")
    return (
        f"{DISCLAIMER}\n\n## âœ”ï¸ ê³µì • ìœˆë„ìš° ê²€ì¦\n"
        f"| íŒŒë¼ë¯¸í„° | ì…ë ¥ê°’ | ë²”ìœ„ | ê²°ê³¼ |\n|----------|--------|------|------|\n" + "\n".join(rows) + "\n\n"
        f"### ìœ„í—˜ íŒŒë¼ë¯¸í„°\n" + ("\n".join(alerts) if alerts else "- ì—†ìŒ")
    )


def analyze_metrics(
    period: str = None,
    metrics_data: str = None,
    targets_data: str = None,
    equipment_id: str = None,
    metrics_dict: Dict[str, float] = None,
    targets: Dict[str, float] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(metrics_data, str) and not metrics_dict:
        metrics_dict = _parse_csv_dict(metrics_data, ',')
    if isinstance(targets_data, str) and not targets:
        targets = _parse_csv_dict(targets_data, ',')
    
    miss = _missing(["metrics_dict", "targets"], {"metrics_dict": metrics_dict, "targets": targets})
    if miss:
        return _err_missing(miss)
    rows = []
    gaps = []
    for k, target in targets.items():
        cur = metrics_dict.get(k)
        status = "âŒ ë¯¸ë‹¬" if cur is None or cur < target else "âœ… ë‹¬ì„±"
        rows.append(f"| {k} | {cur} | {target} | {status} |")
        if cur is None or cur < target:
            gaps.append(f"- {k}: {(cur - target) if cur is not None else 'N/A'}")
    return (
        f"{DISCLAIMER}\n\n## ğŸ“ˆ ë©”íŠ¸ë¦­ ë¶„ì„\n- ê¸°ê°„: {period or 'ë¯¸ì…ë ¥'} / ì¥ë¹„: {equipment_id or 'ì „ì²´'}\n\n"
        f"| ì§€í‘œ | í˜„ì¬ | ëª©í‘œ | ìƒíƒœ |\n|------|------|------|------|\n" + "\n".join(rows)
        + "\n\n### ê°œì„  í•„ìš” í•­ëª©\n"
        + ("\n".join(gaps) if gaps else "- ëª¨ë“  KPI ë‹¬ì„±")
    )


def analyze_spc_data(
    parameter_name: str = None,
    data_points: str = None,
    usl: float = None,
    lsl: float = None,
    target: float = None,
    ucl: float = None,
    lcl: float = None,
    equipment_id: str = None,
    data_points_list: List[float] = None,
    spec_limits: Dict[str, float] = None,
    control_limits: Optional[Dict[str, float]] = None,
    subgroup_size: int = 1,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(data_points, str) and not data_points_list:
        try:
            data_points_list = [float(x.strip()) for x in data_points.split(',') if x.strip()]
        except ValueError:
            return f"{DISCLAIMER}\n\n## âš ï¸ ì…ë ¥ ì˜¤ë¥˜\në°ì´í„° í¬ì¸íŠ¸ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
    if usl is not None and isinstance(usl, str):
        try:
            usl = float(usl)
        except (ValueError, TypeError):
            pass
    if lsl is not None and isinstance(lsl, str):
        try:
            lsl = float(lsl)
        except (ValueError, TypeError):
            pass
    if target is not None and isinstance(target, str):
        try:
            target = float(target)
        except (ValueError, TypeError):
            pass
    
    if not spec_limits and (usl is not None or lsl is not None):
        spec_limits = {'usl': usl, 'lsl': lsl, 'target': target}
    
    miss = _missing(["data_points_list", "spec_limits"], {"data_points_list": data_points_list, "spec_limits": spec_limits})
    if miss:
        return _err_missing(miss)
    if not data_points_list:
        return f"{DISCLAIMER}\n\n## âš ï¸ ì…ë ¥ ì˜¤ë¥˜\në°ì´í„° í¬ì¸íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."

    n = len(data_points_list)
    sample_warning = "âš ï¸ ISO 22514 ê¶Œì¥ ìµœì†Œ ìƒ˜í”Œ ìˆ˜(25ê°œ) ë¯¸ë‹¬. í•´ì„ ì£¼ì˜." if n < 25 else ""
    mean_val = statistics.mean(data_points_list)
    variance = statistics.pvariance(data_points_list) if n > 1 else 0.0
    std_dev = variance ** 0.5

    skewness = (
        sum((x - mean_val) ** 3 for x in data_points_list) / (n * (statistics.pvariance(data_points_list) ** 1.5))
        if n > 2 and std_dev > 0
        else 0.0
    )
    kurtosis = (
        sum((x - mean_val) ** 4 for x in data_points_list) / (n * (variance**2)) - 3
        if n > 3 and variance > 0
        else 0.0
    )
    is_normal = abs(skewness) < 1 and abs(kurtosis) < 2
    normality_warning = "" if is_normal else "âš ï¸ ì •ê·œì„± ë¯¸í¡ ê°€ëŠ¥. Cp/Cpk í•´ì„ ì£¼ì˜."

    A2_TABLE = {2: 1.880, 3: 1.023, 4: 0.729, 5: 0.577, 6: 0.483, 7: 0.419, 8: 0.373, 9: 0.337, 10: 0.308}
    if control_limits is None:
        if subgroup_size == 1:
            mrs = [abs(data_points_list[i] - data_points_list[i - 1]) for i in range(1, n)]
            mr_bar = statistics.mean(mrs) if mrs else 0.0
            d2 = 1.128
            sigma_within = mr_bar / d2 if d2 else std_dev
            ucl = mean_val + 3 * sigma_within
            lcl = mean_val - 3 * sigma_within
            cl = mean_val
            calc_method = "ì´ë™ë²”ìœ„ë²• (MR/d2)"
        else:
            subgroups = [data_points[i : i + subgroup_size] for i in range(0, n, subgroup_size)]
            subgroup_means = [statistics.mean(sg) for sg in subgroups if len(sg) == subgroup_size]
            subgroup_ranges = [(max(sg) - min(sg)) for sg in subgroups if len(sg) == subgroup_size]
            x_bar_bar = statistics.mean(subgroup_means) if subgroup_means else mean_val
            r_bar = statistics.mean(subgroup_ranges) if subgroup_ranges else 0.0
            A2 = A2_TABLE.get(subgroup_size, 0.577)
            ucl = x_bar_bar + A2 * r_bar
            lcl = x_bar_bar - A2 * r_bar
            cl = x_bar_bar
            sigma_within = std_dev
            calc_method = f"X-bar & R (A2={A2}, n={subgroup_size})"
    else:
        ucl = control_limits.get("ucl")
        lcl = control_limits.get("lcl")
        cl = control_limits.get("cl", mean_val)
        sigma_within = std_dev
        calc_method = "ì‚¬ìš©ì ì§€ì •"

    usl, lsl = spec_limits.get("usl"), spec_limits.get("lsl")
    sigma_overall = std_dev or sigma_within or 1e-9
    Cp = (usl - lsl) / (6 * sigma_within) if usl is not None and lsl is not None and sigma_within else None
    Pp = (usl - lsl) / (6 * sigma_overall) if usl is not None and lsl is not None and sigma_overall else None

    def _cpk(mean, usl, lsl, sigma):
        if (usl is None and lsl is None) or sigma == 0:
            return None
        if usl is not None and lsl is not None:
            return min((usl - mean) / (3 * sigma), (mean - lsl) / (3 * sigma))
        if usl is not None:
            return (usl - mean) / (3 * sigma)
        return (mean - lsl) / (3 * sigma)

    Cpk = _cpk(mean_val, usl, lsl, sigma_within)
    Ppk = _cpk(mean_val, usl, lsl, sigma_overall)
    violations = [x for x in data_points_list if (ucl is not None and x > ucl) or (lcl is not None and x < lcl)]
    violations_text = "\n".join(f"- {v:.3f}" for v in violations[:5]) if violations else "- ìœ„ë°˜ ì‚¬í•­ ì—†ìŒ"

    # í¬ë§·íŒ…ì„ ìœ„í•´ ê°’ì„ ë¯¸ë¦¬ ì •ì œ
    ucl_fmt = f"{ucl:.4f}" if ucl is not None else "N/A"
    cl_fmt = f"{cl:.4f}" if cl is not None else "N/A"
    lcl_fmt = f"{lcl:.4f}" if lcl is not None else "N/A"
    cp_fmt = f"{Cp:.3f}" if Cp is not None else "N/A"
    pp_fmt = f"{Pp:.3f}" if Pp is not None else "N/A"
    cpk_fmt = f"{Cpk:.3f}" if Cpk is not None else "N/A"
    ppk_fmt = f"{Ppk:.3f}" if Ppk is not None else "N/A"

    return f"""{DISCLAIMER}

## ğŸ“Š SPC ë¶„ì„ ê²°ê³¼
- íŒŒë¼ë¯¸í„°: {parameter_name or 'ë¯¸ì…ë ¥'} / ì¥ë¹„: {equipment_id or 'ë¯¸ì…ë ¥'}
- ê´€ë¦¬í•œê³„ ê³„ì‚°: {calc_method}
- ìƒ˜í”Œ ìˆ˜: {n}ê°œ {sample_warning}
- ì •ê·œì„±: {"ì •ìƒ" if is_normal else "ì£¼ì˜"} {normality_warning}

### ê¸°ë³¸ í†µê³„ëŸ‰
| í•­ëª© | ê°’ |
|------|-----|
| í‰ê·  (XÌ„) | {mean_val:.4f} |
| í‘œì¤€í¸ì°¨ (Ïƒ) | {std_dev:.4f} |
| ìµœëŒ€/ìµœì†Œ | {max(data_points_list):.4f} / {min(data_points_list):.4f} |

### ê´€ë¦¬í•œê³„
| í•­ëª© | ê°’ |
|------|-----|
| UCL | {ucl_fmt} |
| CL | {cl_fmt} |
| LCL | {lcl_fmt} |

### ê³µì •ëŠ¥ë ¥ì§€ìˆ˜
| ì§€ìˆ˜ | ê°’ |
|------|-----|
| Cp / Pp | {cp_fmt} / {pp_fmt} |
| Cpk / Ppk | {cpk_fmt} / {ppk_fmt} |

### ê´€ë¦¬ í•œê³„ ì´íƒˆ
{violations_text}
"""


def predict_defect_risk(
    process_name: str = None,
    window_params: str = None,
    current_params: str = None,
    severity_params: str = None,
    critical_params: str = None,
    process_window: Dict[str, Dict[str, float]] = None,
    current_conditions: Dict[str, float] = None,
    severity_ratings: Optional[Dict[str, int]] = None,
    occurrence_ratings: Optional[Dict[str, int]] = None,
    detection_ratings: Optional[Dict[str, int]] = None,
    historical_defect_correlation: Optional[Dict[str, str]] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if window_params and not process_window:
        process_window = _parse_window_params(window_params)
    if current_params and not current_conditions:
        current_conditions = _parse_recipe_params(current_params)
    
    critical_list = []
    if isinstance(critical_params, str):
        critical_list = [p.strip() for p in critical_params.split(',')]
    elif isinstance(critical_params, list):
        critical_list = critical_params
    
    if severity_params and not severity_ratings:
        severity_ratings = _parse_csv_dict(severity_params, ',')
    
    miss = _missing(["process_window", "current_conditions"], {"process_window": process_window, "current_conditions": current_conditions})
    if miss:
        return _err_missing(miss)

    severity_ratings = severity_ratings or {}
    occurrence_ratings = occurrence_ratings or {}
    detection_ratings = detection_ratings or {}
    historical_defect_correlation = historical_defect_correlation or {}

    def estimate_occurrence_from_margin(cur, min_v, max_v):
        rng = max_v - min_v
        if rng <= 0:
            return 5, "ë²”ìœ„ ì •ì˜ ë¶ˆëª…í™•"
        min_margin = min(max_v - cur, cur - min_v)
        margin_pct = (min_margin / (rng / 2)) * 100 if rng else 0
        if margin_pct <= 0:
            return 10, "ë²”ìœ„ ì´íƒˆ"
        if margin_pct < 10:
            return 8, "ë§ˆì§„ 10% ë¯¸ë§Œ"
        if margin_pct < 20:
            return 6, "ë§ˆì§„ 20% ë¯¸ë§Œ"
        if margin_pct < 30:
            return 5, "ë§ˆì§„ 30% ë¯¸ë§Œ"
        if margin_pct < 50:
            return 3, "ë§ˆì§„ 50% ë¯¸ë§Œ"
        return 2, "ë§ˆì§„ ì—¬ìœ "

    def action_priority(s, o, d, rpn):
        if s >= 9 and o >= 4:
            return "H"
        if s >= 5 and o >= 6:
            return "H"
        if s >= 5 and o >= 4 and d >= 6:
            return "M"
        if rpn >= 100:
            return "M"
        return "L"

    rows = []
    rpns = []
    for p, limits in process_window.items():
        cur = current_conditions.get(p)
        min_v, max_v = limits.get("min"), limits.get("max")
        if cur is None or min_v is None or max_v is None:
            continue
        S = severity_ratings.get(p, 5)
        if p in occurrence_ratings:
            O = occurrence_ratings[p]
            o_basis = "ì‚¬ìš©ì ì…ë ¥"
        else:
            O, o_basis = estimate_occurrence_from_margin(cur, min_v, max_v)
        D = detection_ratings.get(p, 5)
        RPN = S * O * D
        AP = action_priority(S, O, D, RPN)
        margin_pct = (min(max_v - cur, cur - min_v) / ((max_v - min_v) / 2)) * 100 if max_v != min_v else 0
        rows.append(
            f"| {p} | {cur} | {margin_pct:.1f}% | {S} | {O} | {D} | {RPN} | {AP} | {o_basis} | {historical_defect_correlation.get(p,'N/A')} |"
        )
        rpns.append(RPN)

    rows_text = "\n".join(rows) if rows else "| - | - | - | - | - | - | - | - | - | - |"
    max_rpn = max(rpns) if rpns else 0
    high_ap = " H " in rows_text
    if high_ap or max_rpn >= 200:
        overall = "ğŸ”´ ê³ ìœ„í—˜ - ì¦‰ì‹œ ì¡°ì¹˜"
    elif max_rpn >= 100:
        overall = "ğŸŸ¡ ì¤‘ìœ„í—˜ - ê³„íš ì¡°ì¹˜"
    else:
        overall = "ğŸŸ¢ ì €ìœ„í—˜ - ëª¨ë‹ˆí„°ë§"

    return f"""{DISCLAIMER}

## ğŸ” FMEA ê¸°ë°˜ ë¶ˆëŸ‰ ìœ„í—˜ë„ í‰ê°€
- ì ìš© ê¸°ì¤€: AIAG & VDA FMEA (ë‹¨ìˆœí™”)
- ì¤‘ìš” íŒŒë¼ë¯¸í„°: {', '.join(critical_params) if critical_params else 'ë¯¸ì…ë ¥'}

### ì¢…í•© ìœ„í—˜ë„
{overall}
- ìµœëŒ€ RPN: {max_rpn}

### íŒŒë¼ë¯¸í„°ë³„ FMEA í…Œì´ë¸”
| íŒŒë¼ë¯¸í„° | í˜„ì¬ê°’ | ë§ˆì§„ | S | O | D | RPN | AP | O ê·¼ê±° | ê³¼ê±° ìƒê´€ |
|----------|--------|------|---|---|---|-----|----|--------|-----------|
{rows_text}
"""


def optimize_recipe_direction(
    current_recipe: Optional[Dict[str, float]] = None,
    recipe_csv: str = None,
    current_performance: Optional[Dict[str, float]] = None,
    perf_csv: str = None,
    target_performance: Optional[Dict[str, float]] = None,
    target_csv: str = None,
    param_sensitivity: Optional[Dict[str, str]] = None,
    constraints: Optional[Dict[str, Dict[str, float]]] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(recipe_csv, str) and not current_recipe:
        current_recipe = _parse_recipe_params(recipe_csv)
    if isinstance(perf_csv, str) and not current_performance:
        current_performance = _parse_csv_dict(perf_csv, ':')
        current_performance = {k: float(v) for k,v in current_performance.items()}
    if isinstance(target_csv, str) and not target_performance:
        target_performance = _parse_csv_dict(target_csv, ':')
        target_performance = {k: float(v) for k,v in target_performance.items()}
    
    miss = _missing(["current_recipe", "current_performance", "target_performance"], 
                    {"current_recipe": current_recipe, "current_performance": current_performance, "target_performance": target_performance})
    if miss:
        return _err_missing(miss)
    param_sensitivity = param_sensitivity or {}
    constraints = constraints or {}
    perf_gaps = [(k, target - current_performance.get(k, 0)) for k, target in target_performance.items()]
    perf_text = "\n".join([f"- {k}: ëª©í‘œ ëŒ€ë¹„ {gap:+.2f}" for k, gap in perf_gaps]) or "- ë°ì´í„° ë¶€ì¡±"
    adjustments = []
    for p, _ in current_recipe.items():
        sens = param_sensitivity.get(p, "MEDIUM")
        cons = constraints.get(p, {})
        min_c, max_c = cons.get("min"), cons.get("max")
        direction = "ìƒí–¥" if any(gap > 0 for _, gap in perf_gaps) else "í•˜í–¥/ìµœì í™”"
        adjustments.append(f"- {p}: {direction} (ë¯¼ê°ë„ {sens}, ì œì•½ {min_c}-{max_c})")
    return (
        f"{DISCLAIMER}\n\n## âš™ï¸ ë ˆì‹œí”¼ ìµœì í™” ë°©í–¥\n"
        f"### ì„±ê³¼ ê°­ ë¶„ì„\n{perf_text}\n\n"
        f"### ì¡°ì • ê¶Œì¥ íŒŒë¼ë¯¸í„°\n" + "\n".join(adjustments or ["- ì…ë ¥ëœ íŒŒë¼ë¯¸í„° ì—†ìŒ"])
    )


def simulate_parameter_change(
    current_state: Optional[Dict[str, Any]] = None,
    state_csv: str = None,
    proposed_changes: Optional[Dict[str, float]] = None,
    changes_csv: str = None,
    impact_rules: Optional[List[Dict[str, Any]]] = None,
    rules_csv: str = None,
    process_window: Optional[Dict[str, Dict[str, float]]] = None,
    window_csv: str = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(state_csv, str) and not current_state:
        # state_csv: "recipe:temp:65,pressure:30;performance:yield:97,cpk:1.2"
        current_state = {}
        for section in state_csv.split(';'):
            section = section.strip()
            if ':' in section:
                key, values = section.split(':', 1)
                current_state[key.strip()] = {k.strip(): float(v.strip()) 
                                             for k,v in (p.split(':') for p in values.split(',') if ':' in p)}
    if isinstance(changes_csv, str) and not proposed_changes:
        proposed_changes = _parse_recipe_params(changes_csv)
    if isinstance(rules_csv, str) and not impact_rules:
        # rules_csv: "time->etch_rate:-10;time->uniformity:-2" (ìƒˆ í¬ë§·)
        # ë˜ëŠ”: "rule1:yield:+2,cpk:+0.1;rule2:yield:-1" (ê¸°ì¡´ í¬ë§·)
        impact_rules = []
        for rule in rules_csv.split(';'):
            rule = rule.strip()
            if not rule:
                continue
            # í™”ì‚´í‘œ í¬ë§· ì²˜ë¦¬: "source->target:effect"
            if '->' in rule and ':' in rule:
                try:
                    source_target, impact_str = rule.rsplit(':', 1)
                    effect = float(impact_str.strip())
                    target = source_target.split('->')[-1].strip()
                    impact_rules.append({
                        'name': source_target.strip(),
                        'impact': {target: effect}
                    })
                except (ValueError, TypeError, IndexError):
                    pass
            # ê¸°ì¡´ ì½œë¡  í¬ë§· ì²˜ë¦¬: "rule1:etch_rate:-10"
            elif ':' in rule:
                parts = rule.split(':', 1)
                rule_name = parts[0].strip()
                impacts_str = parts[1]
                impacts = {}
                for p in impacts_str.split(','):
                    if ':' in p:
                        k, v = p.split(':', 1)
                        try:
                            impacts[k.strip()] = float(v.strip())
                        except (ValueError, TypeError):
                            pass
                if impacts:
                    impact_rules.append({'name': rule_name, 'impact': impacts})
    if isinstance(window_csv, str) and not process_window:
        process_window = _parse_window_params(window_csv)
    
    miss = _missing(["current_state", "proposed_changes", "impact_rules"], 
                    {"current_state": current_state, "proposed_changes": proposed_changes, "impact_rules": impact_rules})
    if miss:
        return _err_missing(miss)
    before_recipe = current_state.get("recipe", {}) if isinstance(current_state, dict) else {}
    before_perf = current_state.get("performance", {}) if isinstance(current_state, dict) else {}
    after_recipe = {**before_recipe, **proposed_changes}
    predicted_perf = before_perf.copy()
    for rule in (impact_rules or []):
        impacts = rule.get("impact", {})
        for metric, delta in impacts.items():
            predicted_perf[metric] = predicted_perf.get(metric, 0) + delta
    window_alerts = []
    if process_window:
        for p, val in after_recipe.items():
            limits = process_window.get(p, {})
            min_v, max_v = limits.get("min"), limits.get("max")
            if min_v is not None and val < min_v:
                window_alerts.append(f"- {p}: {val} < {min_v}")
            if max_v is not None and val > max_v:
                window_alerts.append(f"- {p}: {val} > {max_v}")
    recipe_table = "\n".join([f"| {k} | {before_recipe.get(k,'-')} | {after_recipe.get(k,'-')} |" for k in after_recipe.keys()])
    perf_table = "\n".join([f"| {k} | {before_perf.get(k,'-')} | {predicted_perf.get(k,'-')} |" for k in predicted_perf.keys()])
    risk_text = "- ë²”ìœ„ ì´ˆê³¼ ì—†ìŒ" if not window_alerts else "ë²”ìœ„ ì´ˆê³¼:\n" + "\n".join(window_alerts)
    return (
        f"{DISCLAIMER}\n\n## ğŸ§ª íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹œë®¬ë ˆì´ì…˜\n"
        f"### ë ˆì‹œí”¼ ë³€ê²½ ì „/í›„\n| íŒŒë¼ë¯¸í„° | Before | After |\n|----------|--------|-------|\n{recipe_table}\n\n"
        f"### ì˜ˆìƒ ì„±ê³¼ ë³€í™”\n| ì§€í‘œ | Before | After |\n|------|--------|-------|\n{perf_table}\n\n"
        f"### ë¦¬ìŠ¤í¬ í‰ê°€\n{risk_text}\n\n"
        f"### âš ï¸ ëª¨ë¸ í•œê³„\n"
        f"- ì‚¬ìš©ì ì •ì˜ ì˜í–¥ ê·œì¹™ì„ ì„ í˜• ê°€ì‚° ì ìš©\n"
        f"- ë¬¼ë¦¬/í™”í•™ ëª¨ë¸ ë° ë¹„ì„ í˜• íš¨ê³¼ ë¯¸í¬í•¨\n"
    )


def calculate_yield_impact(
    baseline_yield: Optional[float] = None,
    parameter_changes: Optional[List[Dict[str, Any]]] = None,
    changes_csv: str = None,
    interaction_effects: Optional[List[Dict[str, Any]]] = None,
    interactions_csv: str = None,
    confidence_level: float = 0.95,
    model_type: str = "linear",
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(changes_csv, str) and not parameter_changes:
        # changes_csv: "temperature:start:65,end:70,sensitivity:0.8;pressure:start:30,end:33,sensitivity:0.1"
        # ë˜ëŠ” ê¸°ì¡´ í¬ë§·: "temperature:65:70:0.8;pressure:30:33:0.1"
        parameter_changes = []
        for item in changes_csv.split(';'):
            item = item.strip()
            if ':' in item:
                parts = [p.strip() for p in item.split(':')]
                if len(parts) >= 2:
                    param = parts[0]
                    # ìƒˆ í¬ë§· ì²˜ë¦¬: param:start:65,end:70,sensitivity:0.8
                    if len(parts) == 2 and ',' in parts[1]:
                        sub_parts = {}
                        for sub in parts[1].split(','):
                            if ':' in sub:
                                k, v = sub.split(':', 1)
                                try:
                                    sub_parts[k.strip()] = float(v.strip())
                                except (ValueError, TypeError):
                                    pass
                        if 'start' in sub_parts and 'end' in sub_parts and 'sensitivity' in sub_parts:
                            parameter_changes.append({
                                'param': param,
                                'from': sub_parts['start'],
                                'to': sub_parts['end'],
                                'yield_sensitivity': sub_parts['sensitivity']
                            })
                    # ê¸°ì¡´ í¬ë§·: "temperature:65:70:0.8"
                    elif len(parts) >= 4:
                        try:
                            parameter_changes.append({
                                'param': parts[0],
                                'from': float(parts[1]),
                                'to': float(parts[2]),
                                'yield_sensitivity': float(parts[3])
                            })
                        except (ValueError, TypeError):
                            pass
    if isinstance(interactions_csv, str) and not interaction_effects:
        # interactions_csv: "tempÃ—pressure:0.01;tempÃ—time:0.005"
        interaction_effects = []
        for item in interactions_csv.split(';'):
            item = item.strip()
            if ':' in item:
                params_str, effect = item.split(':', 1)
                interaction_effects.append({
                    'params': [p.strip() for p in params_str.split('Ã—')],
                    'effect': float(effect)
                })
    
    miss = _missing(["baseline_yield", "parameter_changes"], 
                    {"baseline_yield": baseline_yield, "parameter_changes": parameter_changes})
    if miss:
        return _err_missing(miss)
    linear_effects = []
    total_linear = 0.0
    for change in (parameter_changes or []):
        delta = (change.get("to") - change.get("from")) if change.get("to") is not None and change.get("from") is not None else 0
        sens = change.get("yield_sensitivity", 0)
        effect = delta * sens
        total_linear += effect
        linear_effects.append(
            {"param": change.get("param"), "from": change.get("from"), "to": change.get("to"), "delta": delta, "sens": sens, "effect": effect}
        )
    interaction_total = sum(i.get("effect", 0) for i in (interaction_effects or []))
    predicted_yield = baseline_yield + total_linear + interaction_total
    total_effect = abs(total_linear) + abs(interaction_total)
    uncertainty = total_effect * 0.1 if total_effect > 0 else 0.1
    z = 1.96 if confidence_level >= 0.95 else 2.576
    ci_lower = predicted_yield - z * uncertainty
    ci_upper = predicted_yield + z * uncertainty
    linear_rows = "\n".join(
        f"| {e['param']} | {e['from']} â†’ {e['to']} | {e['delta']:+.2f} | {e['sens']:.4f} | {e['effect']:+.3f}% |"
        for e in linear_effects
    )
    inter_rows = (
        "\n".join([f"| {' Ã— '.join(i.get('params', []))} | {i.get('effect', 0):+.3f}% |" for i in (interaction_effects or [])])
        if interaction_effects
        else "| (ì—†ìŒ) | - |"
    )
    return f"""{DISCLAIMER}

## ğŸ“ˆ ìˆ˜ìœ¨ ì˜í–¥ ë¶„ì„ (DOE ê¸°ë°˜ ë‹¨ìˆœ ì˜ˆì¸¡)
- ëª¨ë¸: {model_type}
- ì‹ ë¢°ìˆ˜ì¤€: {confidence_level*100:.0f}%

### ê²°ê³¼
| í•­ëª© | ê°’ |
|------|-----|
| ê¸°ì¤€ ìˆ˜ìœ¨ | {baseline_yield:.2f}% |
| ì„ í˜• íš¨ê³¼ í•©ê³„ | {total_linear:+.3f}% |
| ìƒí˜¸ì‘ìš© í•©ê³„ | {interaction_total:+.3f}% |
| ì˜ˆì¸¡ ìˆ˜ìœ¨ | **{predicted_yield:.2f}%** |
| ì‹ ë¢°êµ¬ê°„ | [{ci_lower:.2f}%, {ci_upper:.2f}%] |

### ì„ í˜• íš¨ê³¼
| íŒŒë¼ë¯¸í„° | ë³€ê²½ | Î” | ë¯¼ê°ë„ | ì˜í–¥ |
|----------|------|---|--------|------|
{linear_rows}

### ìƒí˜¸ì‘ìš© íš¨ê³¼
| íŒŒë¼ë¯¸í„° ì¡°í•© | ì˜í–¥ |
|---------------|------|
{inter_rows}
"""


def analyze_equipment_comparison(
    equipment_data: Optional[List[Dict[str, Any]]] = None,
    equipment_list: str = None,
    metrics_data: str = None,
    weights_csv: str = None,
    benchmark_csv: str = None,
    normalization_method: str = "min-max",
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(equipment_list, str) and not equipment_data:
        equipment_data = []
        eq_list = [e.strip() for e in equipment_list.split(',') if e.strip()]
        # metrics_data: "ETCH-01:yield:98.5,cpk:1.45;ETCH-02:yield:97.2,cpk:1.28"
        if metrics_data:
            for item in metrics_data.split(';'):
                item = item.strip()
                if ':' in item:
                    eq_id, metrics_str = item.split(':', 1)
                    metrics = {}
                    for metric_pair in metrics_str.split(','):
                        metric_pair = metric_pair.strip()
                        if ':' in metric_pair:
                            k, v = metric_pair.split(':', 1)
                            try:
                                metrics[k.strip()] = float(v.strip())
                            except ValueError:
                                pass
                    equipment_data.append({'equipment_id': eq_id.strip(), 'metrics': metrics})
    weights = None
    benchmark = None
    if isinstance(weights_csv, str) and not weights:
        weights = _parse_csv_dict(weights_csv, ':')
        weights = {k: float(v) for k,v in weights.items()}
    if isinstance(benchmark_csv, str) and not benchmark:
        benchmark = _parse_csv_dict(benchmark_csv, ':')
        benchmark = {k: float(v) for k,v in benchmark.items()}
    
    miss = _missing(["equipment_data"], {"equipment_data": equipment_data})
    if miss:
        return _err_missing(miss)

    all_metrics = set()
    for eq in equipment_data:
        all_metrics.update(eq.get("metrics", {}).keys())
    if not all_metrics:
        return f"{DISCLAIMER}\n\n## âš ï¸ ì…ë ¥ ì˜¤ë¥˜\në©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤."

    if weights is None:
        weights = {m: 1 / len(all_metrics) for m in all_metrics}
    wsum = sum(weights.values()) or 1.0
    weight_warning = ""
    if abs(wsum - 1.0) > 0.01:
        weights = {k: v / wsum for k, v in weights.items()}
        weight_warning = f"âš ï¸ ê°€ì¤‘ì¹˜ í•©ê³„ {wsum:.2f} â†’ ì •ê·œí™” ì ìš©"

    higher_is_better = {"yield": True, "cpk": True, "uptime": True, "throughput": True, "mtbf": True, "defect_rate": False, "mttr": False}

    def norm_minmax(vals, higher=True):
        mn, mx = min(vals), max(vals)
        if mx == mn:
            return [0.5] * len(vals)
        return [((v - mn) / (mx - mn)) if higher else ((mx - v) / (mx - mn)) for v in vals]

    normalized = []
    for eq in equipment_data:
        norm_metrics = {}
        for m in all_metrics:
            vals = [e.get("metrics", {}).get(m, 0) for e in equipment_data]
            higher = higher_is_better.get(m, True)
            norm_vals = norm_minmax(vals, higher)
            norm_metrics[m] = norm_vals[equipment_data.index(eq)]
        score = sum(norm_metrics.get(m, 0) * weights.get(m, 0) for m in all_metrics) * 100
        normalized.append({"equipment_id": eq.get("equipment_id", "?"), "orig": eq.get("metrics", {}), "score": score})

    normalized.sort(key=lambda x: x["score"], reverse=True)
    for i, eq in enumerate(normalized):
        eq["rank"] = i + 1

    metric_headers = " | ".join(all_metrics)
    rows = "\n".join(
        f"| {eq['rank']} | {eq['equipment_id']} | {eq['score']:.1f} | " + " | ".join(str(eq["orig"].get(m, "N/A")) for m in all_metrics) + " |"
        for eq in normalized
    )

    bm_text = ""
    if benchmark:
        bm_rows = []
        for eq in normalized:
            checks = []
            for m, bm in benchmark.items():
                val = eq["orig"].get(m)
                if val is None:
                    checks.append("â“")
                    continue
                better = higher_is_better.get(m, True)
                ok = val >= bm if better else val <= bm
                checks.append("âœ…" if ok else "âŒ")
            bm_rows.append(f"| {eq['equipment_id']} | {' '.join(checks)} |")
        bm_text = "\n### ë²¤ì¹˜ë§ˆí¬ ë¹„êµ\n| ì¥ë¹„ | ì¶©ì¡± |\n|------|------|\n" + "\n".join(bm_rows)

    return f"""{DISCLAIMER}

## ğŸ­ ì¥ë¹„ ì„±ëŠ¥ ë¹„êµ (ê°€ì¤‘í•©)
- ì •ê·œí™”: {normalization_method.upper()}
- ê°€ì¤‘ì¹˜ í•©ê³„ ê²€ì¦: {weight_warning or 'OK'}

### ê°€ì¤‘ì¹˜
| ì§€í‘œ | ê°€ì¤‘ì¹˜ |
|------|--------|
{chr(10).join([f"| {m} | {w:.2%} |" for m, w in weights.items()])}

### ìˆœìœ„
| ìˆœìœ„ | ì¥ë¹„ | ì ìˆ˜ | {metric_headers} |
|------|------|------|{' | '.join(['---']*len(all_metrics))}|
{rows}
{bm_text}
"""


def generate_shift_report(
    shift_info: str = None,
    production_data: str = None,
    equipment_status: str = None,
    quality_data: str = None,
    events: str = None,
    pending_actions: str = None,
    shift_info_dict: Optional[Dict[str, str]] = None,
    production_summary: Dict[str, Any] = None,
    equipment_status_list: List[Dict[str, Any]] = None,
    quality_summary: Dict[str, Any] = None,
    key_events: Optional[List[Dict[str, Any]]] = None,
    pending_actions_list: Optional[List[str]] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(production_data, str) and not production_summary:
        production_summary = _parse_csv_dict(production_data, ',')
    if isinstance(equipment_status, str) and not equipment_status_list:
        equipment_status_list = []
        for item in equipment_status.split(';'):
            item = item.strip()
            parts = [p.strip() for p in item.split(':')]
            if len(parts) >= 2:
                equipment_status_list.append({'equipment_id': parts[0], 'status': parts[1], 'issues': parts[2] if len(parts) > 2 else '-'})
    if isinstance(quality_data, str) and not quality_summary:
        quality_summary = _parse_csv_dict(quality_data, ',')
    if isinstance(pending_actions, str) and not pending_actions_list:
        pending_actions_list = [p.strip() for p in pending_actions.split(';') if p.strip()]
    if isinstance(events, str) and not key_events:
        key_events = []
        for item in events.split(';'):
            item = item.strip()
            if ' ' in item:
                parts = item.split(' ', 1)
                key_events.append({'time': parts[0], 'event': parts[1]})
    
    miss = _missing(["production_summary", "equipment_status_list", "quality_summary"], 
                    {"production_summary": production_summary, "equipment_status_list": equipment_status_list, "quality_summary": quality_summary})
    if miss:
        return _err_missing(miss)
    
    eq_rows = "\n".join(
        [f"| {e.get('equipment_id','-')} | {e.get('status','-')} | {e.get('issues','-')} |" for e in equipment_status_list]
    ) or "| - | - | - |"
    events_rows = "\n".join(
        [f"| {ev.get('time','-')} | {ev.get('event','-')} | {ev.get('action','-')} | {ev.get('status','-')} |" for ev in (key_events or [])]
    ) or "| - | - | - | - |"
    pending = "\n".join([f"- {p}" for p in (pending_actions_list or [])]) or "- ë¯¸ê²° ì—†ìŒ"
    shift_str = f"{shift_info}" if shift_info else "ë¯¸ì…ë ¥"
    
    return (
        f"{DISCLAIMER}\n\n## ğŸ“ êµëŒ€ ë¦¬í¬íŠ¸\n"
        f"- **êµëŒ€ ì •ë³´**: {shift_str}\n\n"
        f"### ìƒì‚° ìš”ì•½\n"
        f"- íˆ¬ì…: {production_summary.get('in','-')}\n"
        f"- ì™„ë£Œ: {production_summary.get('out','-')}\n"
        f"- ëª©í‘œ: {production_summary.get('target','-')}\n"
        f"- ìˆ˜ìœ¨: {production_summary.get('yield','-')}\n\n"
        f"### ì¥ë¹„ ìƒíƒœ\n| ì¥ë¹„ | ìƒíƒœ | ì´ìŠˆ |\n|------|------|------|\n{eq_rows}\n\n"
        f"### í’ˆì§ˆ ìš”ì•½\n"
        f"- ë¶ˆëŸ‰ ìˆ˜: {quality_summary.get('defects','-')}\n"
        f"- ì£¼ìš” ë¶ˆëŸ‰: {quality_summary.get('major','-')}\n\n"
        f"### ì£¼ìš” ì´ë²¤íŠ¸\n| ì‹œê°„ | ì´ë²¤íŠ¸ | ì¡°ì¹˜ | ìƒíƒœ |\n|------|--------|------|------|\n{events_rows}\n\n"
        f"### ì¸ìˆ˜ì¸ê³„ í•„ìš” ì‚¬í•­\n{pending}\n"
    )


def analyze_trend(
    parameter_name: str = None,
    data_points: str = None,
    timestamps: str = None,
    usl: float = None,
    lsl: float = None,
    forecast_count: int = 0,
    time_series_data: List[Dict[str, Any]] = None,
    spec_limits: Optional[Dict[str, float]] = None,
    analysis_options: Optional[Dict[str, Any]] = None,
) -> str:
    # CSV í˜•ì‹ ì§€ì›
    if isinstance(data_points, str) and not time_series_data:
        try:
            values = [float(x.strip()) for x in data_points.split(',') if x.strip()]
            time_series_data = [{"value": v} for v in values]
        except ValueError:
            return f"{DISCLAIMER}\n\n## âš ï¸ ì…ë ¥ ì˜¤ë¥˜\në°ì´í„° í¬ì¸íŠ¸ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    if not spec_limits and (usl is not None or lsl is not None):
        spec_limits = {'usl': usl, 'lsl': lsl}
    
    if forecast_count and not analysis_options:
        analysis_options = {'forecast_points': forecast_count}
    
    miss = _missing(["time_series_data", "parameter_name"], {"time_series_data": time_series_data, "parameter_name": parameter_name})
    if miss:
        return _err_missing(miss)
    values = [d.get("value") for d in time_series_data if d.get("value") is not None]
    n = len(values)
    if n < 5:
        return f"{DISCLAIMER}\n\n## âš ï¸ ë°ì´í„° ë¶€ì¡±\në¶„ì„ì„ ìœ„í•´ ìµœì†Œ 5ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬: {n}ê°œ)"

    analysis_options = analysis_options or {}
    forecast_points = analysis_options.get("forecast_points", 0)
    mean_val = sum(values) / n
    variance = sum((x - mean_val) ** 2 for x in values) / (n - 1)
    std_dev = variance**0.5

    x = list(range(n))
    x_mean = sum(x) / n
    y_mean = mean_val
    denom = sum((xi - x_mean) ** 2 for xi in x)
    slope = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n)) / denom if denom else 0
    intercept = y_mean - slope * x_mean
    ss_tot = sum((v - y_mean) ** 2 for v in values)
    ss_res = sum((values[i] - (slope * x[i] + intercept)) ** 2 for i in range(n))
    r_squared = 1 - (ss_res / ss_tot) if ss_tot else 0
    if n > 2 and denom > 0:
        mse = ss_res / (n - 2)
        se_slope = (mse / denom) ** 0.5 if denom else 0
        t_stat = slope / se_slope if se_slope else 0
        t_crit = 2.0 if n > 30 else 2.3
        trend_significant = abs(t_stat) > t_crit
    else:
        t_stat = 0
        trend_significant = False

    def mann_kendall(data):
        s = 0
        for i in range(len(data) - 1):
            for j in range(i + 1, len(data)):
                diff = data[j] - data[i]
                if diff > 0:
                    s += 1
                elif diff < 0:
                    s -= 1
        var_s = (n * (n - 1) * (2 * n + 5)) / 18
        if s > 0:
            z = (s - 1) / (var_s**0.5)
        elif s < 0:
            z = (s + 1) / (var_s**0.5)
        else:
            z = 0
        trend_txt = "ìƒìŠ¹ ì¶”ì„¸(ìœ ì˜)" if z > 1.96 else "í•˜ë½ ì¶”ì„¸(ìœ ì˜)" if z < -1.96 else "ì¶”ì„¸ ì—†ìŒ(ë¹„ìœ ì˜)"
        return {"S": s, "Z": z, "trend": trend_txt, "significant": abs(z) > 1.96}

    mk = mann_kendall(values)
    first_half = values[: n // 2]
    second_half = values[n // 2 :]
    shift_size = 0
    shift_status = "ì‹œí”„íŠ¸ ì—†ìŒ"
    if first_half and second_half and std_dev > 0:
        shift_size = (sum(second_half) / len(second_half) - sum(first_half) / len(first_half)) / std_dev
        if abs(shift_size) > 1.5:
            shift_status = "ìœ ì˜í•œ ì‹œí”„íŠ¸"
        elif abs(shift_size) > 1.0:
            shift_status = "ì‹œí”„íŠ¸ ì˜ì‹¬"

    forecast_text = ""
    if forecast_points > 0:
        forecasts = []
        for i in range(forecast_points):
            fx = n + i
            fy = slope * fx + intercept
            forecasts.append(f"| +{i+1} | {fy:.4f} |")
        forecast_text = "\n### ì˜ˆì¸¡ (ì„ í˜• ì™¸ì‚½)\n| ì‹œì  | ì˜ˆì¸¡ê°’ |\n|------|--------|\n" + "\n".join(forecasts) + "\n"

    if mk["significant"] and trend_significant:
        conclusion = "ğŸ”´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì¶”ì„¸"
        action = "ì›ì¸ íŒŒì•… ë° ì¡°ì¹˜"
    elif mk["significant"] or trend_significant:
        conclusion = "ğŸŸ¡ ì¶”ì„¸ ì˜ì‹¬"
        action = "ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘/ëª¨ë‹ˆí„°ë§"
    else:
        conclusion = "ğŸŸ¢ ìœ ì˜í•œ ì¶”ì„¸ ì—†ìŒ"
        action = "í˜„ìƒ ìœ ì§€"

    spec_txt = ""
    if spec_limits:
        usl, lsl = spec_limits.get("usl"), spec_limits.get("lsl")
        out = [v for v in values if (usl is not None and v > usl) or (lsl is not None and v < lsl)]
        spec_txt = f"- ìŠ¤í™ ì´íƒˆ: {len(out)}ê±´"

    return f"""{DISCLAIMER}

## ğŸ“ˆ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
- ëŒ€ìƒ: {parameter_name}
- ë°ì´í„° ìˆ˜: {n}ê°œ
{spec_txt}

### ê¸°ë³¸ í†µê³„ëŸ‰
| í•­ëª© | ê°’ |
|------|-----|
| í‰ê·  | {mean_val:.4f} |
| í‘œì¤€í¸ì°¨ | {std_dev:.4f} |
| ìµœëŒ€/ìµœì†Œ | {max(values):.4f} / {min(values):.4f} |

### ì„ í˜• íšŒê·€(OLS)
| í•­ëª© | ê°’ | í•´ì„ |
|------|-----|------|
| ê¸°ìš¸ê¸° | {slope:.6f} | {'ì–‘(ìƒìŠ¹)' if slope>0 else 'ìŒ(í•˜ë½)' if slope<0 else '0'} |
| RÂ² | {r_squared:.4f} | |
| t-í†µê³„ëŸ‰ | {t_stat:.3f} | ìœ ì˜ì„±: {"ìœ ì˜" if trend_significant else "ë¹„ìœ ì˜"} |

### Mann-Kendall
| S | Z | íŒì • |
|---|---|------|
| {mk['S']} | {mk['Z']:.3f} | {mk['trend']} |

### ì‹œí”„íŠ¸ íƒì§€
| í•­ëª© | ê°’ |
|------|-----|
| ì‹œí”„íŠ¸ í¬ê¸°(Ïƒ) | {shift_size:.2f} |
| íŒì • | {shift_status} |

### ì¢…í•© íŒì •
- ê²°ë¡ : {conclusion}
- ê¶Œì¥ ì¡°ì¹˜: {action}
{forecast_text}
"""


TOOLS = [
    # 1. analyze_defect - ê¸°ì¡´ ìœ ì§€ (ì´ë¯¸ ë‹¨ìˆœí•¨)
    {
        "name": "analyze_defect",
        "description": "ë°˜ë„ì²´ ì›¨ì´í¼ ë¶ˆëŸ‰ì„ ë¶„ì„í•©ë‹ˆë‹¤. ë¶ˆëŸ‰ ì½”ë“œ, ì„¤ëª…, ê³µì • ë‹¨ê³„ë¥¼ ì…ë ¥í•˜ë©´ ì›ì¸ ë¶„ì„ê³¼ ì ê²€ í•­ëª©ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "defect_code": {
                    "type": "string",
                    "description": "ë¶ˆëŸ‰ ì½”ë“œ. ì˜ˆ: PARTICLE, SCRATCH, CD_VARIATION"
                },
                "defect_description": {
                    "type": "string",
                    "description": "ë¶ˆëŸ‰ ìƒì„¸ ì„¤ëª…. ì˜ˆ: ì›¨ì´í¼ ê°€ì¥ìë¦¬ ê¹Šì€ ìŠ¤í¬ë˜ì¹˜"
                },
                "process_step": {
                    "type": "string",
                    "description": "ê³µì • ë‹¨ê³„. ì˜ˆ: ETCH, CVD, CMP, LITHO"
                },
                "equipment_id": {
                    "type": "string",
                    "description": "ì¥ë¹„ ID (ì„ íƒ). ì˜ˆ: CMP-01"
                },
                "wafer_id": {
                    "type": "string",
                    "description": "ì›¨ì´í¼ ID (ì„ íƒ). ì˜ˆ: W123"
                },
                "known_causes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ì•Œë ¤ì§„ ì›ì¸ ëª©ë¡ (ì„ íƒ). ì˜ˆ: ['íŒ¨ë“œ ë§ˆëª¨', 'ìŠ¬ëŸ¬ë¦¬ ì˜¤ì—¼']"
                },
                "recent_changes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ìµœê·¼ ë³€ê²½ ì‚¬í•­ (ì„ íƒ). ì˜ˆ: ['ìŠ¬ëŸ¬ë¦¬ Lot êµì²´', 'PM ìˆ˜í–‰']"
                }
            },
            "required": ["defect_code", "defect_description", "process_step"]
        }
    },
    
    # 2. get_defect_history - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "get_defect_history",
        "description": "ë¶ˆëŸ‰ ì´ë ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ë¶ˆëŸ‰ ìœ í˜•ê³¼ ì´ë ¥ ë°ì´í„°ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ë©´ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "defect_type": {
                    "type": "string",
                    "description": "ë¶ˆëŸ‰ ìœ í˜•. ì˜ˆ: SCRATCH, PARTICLE"
                },
                "records_csv": {
                    "type": "string",
                    "description": "ì´ë ¥ ë°ì´í„° CSV í˜•ì‹: 'ë‚ ì§œ,ì¥ë¹„,ìˆ˜ëŸ‰,ì¡°ì¹˜,ê²°ê³¼' ê° í–‰ì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: '2025-01-10,CMP-01,3,íŒ¨ë“œêµì²´,í•´ê²°;2025-01-05,CMP-02,2,í•„í„°êµì²´,í•´ê²°'"
                },
                "analysis_type": {
                    "type": "string",
                    "enum": ["trend", "equipment", "time"],
                    "description": "ë¶„ì„ ìœ í˜• (ì„ íƒ). ê¸°ë³¸ê°’: trend"
                }
            },
            "required": ["defect_type", "records_csv"]
        }
    },
    
    # 3. suggest_corrective_action - ê¸°ì¡´ ìœ ì§€ (ì´ë¯¸ ë‹¨ìˆœí•¨)
    {
        "name": "suggest_corrective_action",
        "description": "ë¬¸ì œ ìƒí™©ì— ëŒ€í•œ ì‹œì • ì¡°ì¹˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "problem_description": {
                    "type": "string",
                    "description": "ë¬¸ì œ ì„¤ëª…. ì˜ˆ: ì••ë ¥ ë¶ˆì•ˆì •"
                },
                "affected_equipment": {
                    "type": "string",
                    "description": "ì˜í–¥ ë°›ì€ ì¥ë¹„. ì˜ˆ: ETCH-01"
                },
                "severity": {
                    "type": "string",
                    "enum": ["critical", "major", "minor"],
                    "description": "ì‹¬ê°ë„"
                },
                "current_status": {
                    "type": "string",
                    "description": "í˜„ì¬ ìƒíƒœ. ì˜ˆ: ì•ŒëŒ ë°˜ë³µ ë°œìƒ"
                },
                "available_resources": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ê°€ìš© ìì› (ì„ íƒ). ì˜ˆ: ['ì—”ì§€ë‹ˆì–´ 1ëª…', 'í•„í„° ì˜ˆë¹„']"
                },
                "time_constraint": {
                    "type": "string",
                    "description": "ì‹œê°„ ì œì•½ (ì„ íƒ). ì˜ˆ: 4ì‹œê°„ ë‚´"
                }
            },
            "required": ["problem_description", "affected_equipment", "severity", "current_status"]
        }
    },
    
    # 4. compare_to_baseline - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "compare_to_baseline",
        "description": "ê¸°ì¤€ ë ˆì‹œí”¼ì™€ í˜„ì¬ ë ˆì‹œí”¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "recipe_name": {
                    "type": "string",
                    "description": "ë ˆì‹œí”¼ ì´ë¦„. ì˜ˆ: Oxide Etch"
                },
                "baseline_params": {
                    "type": "string",
                    "description": "ê¸°ì¤€ ë ˆì‹œí”¼ CSV í˜•ì‹: 'íŒŒë¼ë¯¸í„°:í‘œì¤€ê°’:ìµœì†Œ:ìµœëŒ€:ë‹¨ìœ„' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:60:55:65:C,pressure:30:25:35:mTorr'"
                },
                "current_params": {
                    "type": "string",
                    "description": "í˜„ì¬ ë ˆì‹œí”¼ CSV í˜•ì‹: 'íŒŒë¼ë¯¸í„°:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:67,pressure:28'"
                }
            },
            "required": ["recipe_name", "baseline_params", "current_params"]
        }
    },
    
    # 5. compare_two_recipes - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "compare_two_recipes",
        "description": "ë‘ ë ˆì‹œí”¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "recipe_a_name": {
                    "type": "string",
                    "description": "ë ˆì‹œí”¼ A ì´ë¦„. ì˜ˆ: Aë¼ì¸"
                },
                "recipe_a_params": {
                    "type": "string",
                    "description": "ë ˆì‹œí”¼ A íŒŒë¼ë¯¸í„° CSV: 'íŒŒë¼ë¯¸í„°:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:60,pressure:30,rf_power:800'"
                },
                "recipe_b_name": {
                    "type": "string",
                    "description": "ë ˆì‹œí”¼ B ì´ë¦„. ì˜ˆ: Bë¼ì¸"
                },
                "recipe_b_params": {
                    "type": "string",
                    "description": "ë ˆì‹œí”¼ B íŒŒë¼ë¯¸í„° CSV: 'íŒŒë¼ë¯¸í„°:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:62,pressure:28,rf_power:820'"
                },
                "tolerance_params": {
                    "type": "string",
                    "description": "í—ˆìš© í¸ì°¨ CSV (ì„ íƒ): 'íŒŒë¼ë¯¸í„°:í¼ì„¼íŠ¸' ì‰¼í‘œë¡œ êµ¬ë¶„. % ê¸°í˜¸ ì„ íƒ. ì˜ˆ: 'temperature:5,pressure:10' ë˜ëŠ” 'temperature:5%,pressure:10%'"
                }
            },
            "required": ["recipe_a_name", "recipe_a_params", "recipe_b_name", "recipe_b_params"]
        }
    },
    
    # 6. validate_process_window - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "validate_process_window",
        "description": "ê³µì • ìœˆë„ìš° ë‚´ ì¡°ê±´ì¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "process_name": {
                    "type": "string",
                    "description": "ê³µì • ì´ë¦„. ì˜ˆ: CVD"
                },
                "window_params": {
                    "type": "string",
                    "description": "ê³µì • ìœˆë„ìš° CSV: 'íŒŒë¼ë¯¸í„°:ìµœì†Œ:ìµœëŒ€' ë˜ëŠ” 'íŒŒë¼ë¯¸í„°:ìµœì†Œ-ìµœëŒ€' í˜•ì‹. ì˜ˆ: 'temperature:450:500,pressure:0.5:1.5' ë˜ëŠ” 'temperature:450-500,pressure:0.5-1.5'"
                },
                "test_params": {
                    "type": "string",
                    "description": "ê²€ì¦í•  ì¡°ê±´ CSV: 'íŒŒë¼ë¯¸í„°:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:480,pressure:0.8'"
                },
                "critical_params": {
                    "type": "string",
                    "description": "ì¤‘ìš” íŒŒë¼ë¯¸í„° (ì„ íƒ): ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature'"
                }
            },
            "required": ["process_name", "window_params", "test_params"]
        }
    },
    
    # 7. analyze_spc_data - ë‹¨ìˆœ íŒŒë¼ë¯¸í„°
    {
        "name": "analyze_spc_data",
        "description": "SPC ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ (ISO 22514 ê¸°ì¤€). ì¸¡ì •ê°’ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "parameter_name": {
                    "type": "string",
                    "description": "íŒŒë¼ë¯¸í„° ì´ë¦„. ì˜ˆ: CD, Temperature"
                },
                "data_points": {
                    "type": "string",
                    "description": "ì¸¡ì •ê°’ ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: '45.2,45.8,44.9,46.1,45.5,45.3'"
                },
                "usl": {
                    "type": "number",
                    "description": "ìƒí•œ ìŠ¤í™ (USL). ì˜ˆ: 50"
                },
                "lsl": {
                    "type": "number",
                    "description": "í•˜í•œ ìŠ¤í™ (LSL). ì˜ˆ: 40"
                },
                "target": {
                    "type": "number",
                    "description": "ëª©í‘œê°’ (ì„ íƒ). ë¯¸ì…ë ¥ì‹œ (USL+LSL)/2"
                },
                "ucl": {
                    "type": "number",
                    "description": "ìƒí•œ ê´€ë¦¬í•œê³„ (ì„ íƒ)"
                },
                "lcl": {
                    "type": "number",
                    "description": "í•˜í•œ ê´€ë¦¬í•œê³„ (ì„ íƒ)"
                },
                "equipment_id": {
                    "type": "string",
                    "description": "ì¥ë¹„ ID (ì„ íƒ)"
                }
            },
            "required": ["parameter_name", "data_points", "usl", "lsl"]
        }
    },
    
    # 8. predict_defect_risk - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "predict_defect_risk",
        "description": "FMEA ê¸°ë°˜ ë¶ˆëŸ‰ ìœ„í—˜ë„ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ê³µì • ìœˆë„ìš°ì™€ í˜„ì¬ ì¡°ê±´ì„ CSVë¡œ ì…ë ¥í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "process_name": {
                    "type": "string",
                    "description": "ê³µì • ì´ë¦„. ì˜ˆ: Etch"
                },
                "window_params": {
                    "type": "string",
                    "description": "ê³µì • ìœˆë„ìš° CSV: 'íŒŒë¼ë¯¸í„°:ìµœì†Œ:ìµœëŒ€' ë˜ëŠ” 'íŒŒë¼ë¯¸í„°:ìµœì†Œ-ìµœëŒ€' í˜•ì‹. ì˜ˆ: 'temperature:100:130,pressure:50:100' ë˜ëŠ” 'temperature:100-130,pressure:50-100'"
                },
                "current_params": {
                    "type": "string",
                    "description": "í˜„ì¬ ì¡°ê±´ CSV: 'íŒŒë¼ë¯¸í„°:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:128,pressure:92'"
                },
                "severity_params": {
                    "type": "string",
                    "description": "ì‹¬ê°ë„ CSV (ì„ íƒ): 'íŒŒë¼ë¯¸í„°:1-10ì ìˆ˜' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:8,pressure:5'"
                },
                "critical_params": {
                    "type": "string",
                    "description": "ì¤‘ìš” íŒŒë¼ë¯¸í„° (ì„ íƒ): ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature'"
                }
            },
            "required": ["process_name", "window_params", "current_params"]
        }
    },
    
    # 9. analyze_trend - ë‹¨ìˆœ íŒŒë¼ë¯¸í„°
    {
        "name": "analyze_trend",
        "description": "ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ (ìµœì†ŒììŠ¹ë²•, Mann-Kendall ê²€ì •).",
        "inputSchema": {
            "type": "object",
            "properties": {
                "parameter_name": {
                    "type": "string",
                    "description": "íŒŒë¼ë¯¸í„° ì´ë¦„. ì˜ˆ: temperature"
                },
                "data_points": {
                    "type": "string",
                    "description": "ì¸¡ì •ê°’ ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: '60.1,60.3,60.5,60.8,61.0,61.2'"
                },
                "timestamps": {
                    "type": "string",
                    "description": "ì‹œê°„ ì •ë³´ (ì„ íƒ) ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: '10:00,11:00,12:00'"
                },
                "usl": {
                    "type": "number",
                    "description": "ìƒí•œ ìŠ¤í™ (ì„ íƒ)"
                },
                "lsl": {
                    "type": "number",
                    "description": "í•˜í•œ ìŠ¤í™ (ì„ íƒ)"
                },
                "forecast_count": {
                    "type": "integer",
                    "description": "ì˜ˆì¸¡í•  í¬ì¸íŠ¸ ìˆ˜ (ì„ íƒ). ì˜ˆ: 3"
                }
            },
            "required": ["parameter_name", "data_points"]
        }
    },
    
    # 10. analyze_metrics - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "analyze_metrics",
        "description": "KPI ë©”íŠ¸ë¦­ì„ ëª©í‘œ ëŒ€ë¹„ ë¶„ì„í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "period": {
                    "type": "string",
                    "description": "ë¶„ì„ ê¸°ê°„. ì˜ˆ: 2025ë…„ 1ì›” 2ì£¼ì°¨"
                },
                "metrics_data": {
                    "type": "string",
                    "description": "í˜„ì¬ ë©”íŠ¸ë¦­ CSV: 'ì§€í‘œ:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'yield:97.8,cpk:1.28,uptime:89.5'"
                },
                "targets_data": {
                    "type": "string",
                    "description": "ëª©í‘œê°’ CSV: 'ì§€í‘œ:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'yield:98,cpk:1.33,uptime:90'"
                },
                "equipment_id": {
                    "type": "string",
                    "description": "ì¥ë¹„ ID (ì„ íƒ)"
                }
            },
            "required": ["period", "metrics_data", "targets_data"]
        }
    },
    
    # 11. generate_shift_report - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "generate_shift_report",
        "description": "êµëŒ€ ê·¼ë¬´ ì¸ìˆ˜ì¸ê³„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "shift_info": {
                    "type": "string",
                    "description": "êµëŒ€ ì •ë³´: 'êµëŒ€:ë‚ ì§œ' í˜•ì‹. ì˜ˆ: 'Day:2025-01-15'"
                },
                "production_data": {
                    "type": "string",
                    "description": "ìƒì‚° ë°ì´í„° CSV: 'in:íˆ¬ì…,out:ì™„ë£Œ,target:ëª©í‘œ,yield:ìˆ˜ìœ¨'. ì˜ˆ: 'in:200,out:195,target:200,yield:98.2'"
                },
                "equipment_status": {
                    "type": "string",
                    "description": "ì¥ë¹„ ìƒíƒœ CSV: 'ì¥ë¹„:ìƒíƒœ:ì´ìŠˆ' ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'ETCH-01:running:ì •ìƒ;ETCH-02:down:PMì¤‘'"
                },
                "quality_data": {
                    "type": "string",
                    "description": "í’ˆì§ˆ ë°ì´í„°. ì˜ˆ: 'defects:5,major:íŒŒí‹°í´3ê±´'"
                },
                "events": {
                    "type": "string",
                    "description": "ì£¼ìš” ì´ë²¤íŠ¸ (ì„ íƒ) ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: '14:00 PMì‹œì‘;16:00 ì˜¨ë„ê²½ê³ '"
                },
                "pending_actions": {
                    "type": "string",
                    "description": "ë¯¸ê²° ì¡°ì¹˜ (ì„ íƒ) ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'ETCH-02 ë³µêµ¬í™•ì¸;CVDì˜¨ë„ëª¨ë‹ˆí„°ë§'"
                }
            },
            "required": ["shift_info", "production_data", "equipment_status", "quality_data"]
        }
    },
    
    # 12. analyze_equipment_comparison - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "analyze_equipment_comparison",
        "description": "ì—¬ëŸ¬ ì¥ë¹„ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "equipment_list": {
                    "type": "string",
                    "description": "ì¥ë¹„ ëª©ë¡ ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'ETCH-01,ETCH-02,ETCH-03'"
                },
                "metrics_data": {
                    "type": "string",
                    "description": "ì¥ë¹„ë³„ ë©”íŠ¸ë¦­ CSV: 'ì¥ë¹„:ì§€í‘œ1:ê°’1,ì§€í‘œ2:ê°’2' ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'ETCH-01:yield:98.5,cpk:1.45;ETCH-02:yield:97.2,cpk:1.28'"
                },
                "weights_csv": {
                    "type": "string",
                    "description": "ê°€ì¤‘ì¹˜ (ì„ íƒ) CSV: 'ì§€í‘œ:ê°€ì¤‘ì¹˜' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'yield:0.4,cpk:0.3,uptime:0.3'"
                },
                "benchmark_csv": {
                    "type": "string",
                    "description": "ë²¤ì¹˜ë§ˆí¬ (ì„ íƒ) CSV: 'ì§€í‘œ:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'yield:98,cpk:1.33'"
                }
            },
            "required": ["equipment_list", "metrics_data"]
        }
    },
    
    # 13. optimize_recipe_direction - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "optimize_recipe_direction",
        "description": "ë ˆì‹œí”¼ ìµœì í™” ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "recipe_csv": {
                    "type": "string",
                    "description": "í˜„ì¬ ë ˆì‹œí”¼ CSV: 'íŒŒë¼ë¯¸í„°:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:67,pressure:28,rf_power:800'"
                },
                "perf_csv": {
                    "type": "string",
                    "description": "í˜„ì¬ ì„±ê³¼ CSV: 'ì§€í‘œ:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'yield:97.5,cpk:1.1'"
                },
                "target_csv": {
                    "type": "string",
                    "description": "ëª©í‘œ ì„±ê³¼ CSV: 'ì§€í‘œ:ê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'yield:98.5,cpk:1.33'"
                }
            },
            "required": ["recipe_csv", "perf_csv", "target_csv"]
        }
    },
    
    # 14. simulate_parameter_change - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "simulate_parameter_change",
        "description": "íŒŒë¼ë¯¸í„° ë³€ê²½ì˜ ì˜í–¥ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "state_csv": {
                    "type": "string",
                    "description": "í˜„ì¬ ìƒíƒœ CSV: 'section:key:val,key:val' ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'recipe:temperature:120,time:300,pressure:75;performance:etch_rate:50,uniformity:91,yield:97'"
                },
                "changes_csv": {
                    "type": "string",
                    "description": "ì œì•ˆëœ ë³€ê²½ CSV: 'íŒŒë¼ë¯¸í„°:ìƒˆê°’' ì‰¼í‘œë¡œ êµ¬ë¶„. ì˜ˆ: 'time:250'"
                },
                "rules_csv": {
                    "type": "string",
                    "description": "ì˜í–¥ ê·œì¹™ CSV: 'ë³€ìˆ˜->ê²°ê³¼:íš¨ê³¼' ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'time->etch_rate:-10;time->uniformity:-2' (ë˜ëŠ” ê¸°ì¡´ 'rule1:etch_rate:-10;rule2:uniformity:-2')"
                },
                "window_csv": {
                    "type": "string",
                    "description": "ê³µì • ìœˆë„ìš° (ì„ íƒ) CSV: 'íŒŒë¼ë¯¸í„°:ìµœì†Œ:ìµœëŒ€' ë˜ëŠ” 'íŒŒë¼ë¯¸í„°:ìµœì†Œ-ìµœëŒ€' í˜•ì‹. ì˜ˆ: 'temperature:100:140,pressure:50:100' ë˜ëŠ” 'temperature:100-140,pressure:50-100'"
                }
            },
            "required": ["state_csv", "changes_csv", "rules_csv"]
        }
    },
    
    # 15. calculate_yield_impact - CSV í˜•ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”
    {
        "name": "calculate_yield_impact",
        "description": "íŒŒë¼ë¯¸í„° ë³€ê²½ì— ë”°ë¥¸ ìˆ˜ìœ¨ ì˜í–¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤ (DOE ê¸°ë°˜, ë‹¨ìˆœí™”).",
        "inputSchema": {
            "type": "object",
            "properties": {
                "baseline_yield": {
                    "type": "number",
                    "description": "ê¸°ì¤€ ìˆ˜ìœ¨. ì˜ˆ: 97.5"
                },
                "changes_csv": {
                    "type": "string",
                    "description": "íŒŒë¼ë¯¸í„° ë³€ê²½ CSV: 'íŒŒë¼ë¯¸í„°:start:ì´ì „,end:ìƒˆê°’,sensitivity:ë¯¼ê°ë„' ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'temperature:start:65,end:70,sensitivity:0.8;pressure:start:30,end:33,sensitivity:0.1' (ë˜ëŠ” ê¸°ì¡´ 'temperature:65:70:0.8;pressure:30:33:0.1')"
                },
                "interactions_csv": {
                    "type": "string",
                    "description": "ìƒí˜¸ ì‘ìš© íš¨ê³¼ (ì„ íƒ) CSV: 'íŒŒë¼ë¯¸í„°1Ã—íŒŒë¼ë¯¸í„°2:íš¨ê³¼' ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„. ì˜ˆ: 'temperatureÃ—pressure:-0.2'"
                },
                "confidence_level": {
                    "type": "number",
                    "description": "ì‹ ë¢°ë„ (ì„ íƒ). ê¸°ë³¸ê°’: 0.95"
                },
                "model_type": {
                    "type": "string",
                    "description": "ëª¨ë¸ ìœ í˜• (ì„ íƒ). ê¸°ë³¸ê°’: linear"
                }
            },
            "required": ["baseline_yield", "changes_csv"]
        }
    }
]

TOOL_HANDLERS = {
    "analyze_defect": analyze_defect,
    "get_defect_history": get_defect_history,
    "suggest_corrective_action": suggest_corrective_action,
    "compare_to_baseline": compare_to_baseline,
    "compare_two_recipes": compare_two_recipes,
    "validate_process_window": validate_process_window,
    "analyze_spc_data": analyze_spc_data,
    "predict_defect_risk": predict_defect_risk,
    "analyze_trend": analyze_trend,
    "analyze_metrics": analyze_metrics,
    "generate_shift_report": generate_shift_report,
    "analyze_equipment_comparison": analyze_equipment_comparison,
    "optimize_recipe_direction": optimize_recipe_direction,
    "simulate_parameter_change": simulate_parameter_change,
    "calculate_yield_impact": calculate_yield_impact,
}


@app.get("/")
async def root():
    return {"service": "SemiProcess MCP", "spec": "2026-01-14", "health": "/health", "mcp": "/mcp", "tools_count": len(TOOLS)}


@app.get("/health")
async def health():
    return {"status": "healthy", "service": "SemiProcess MCP", "version": "2.0.0"}


@app.post("/mcp")
async def mcp_endpoint(request: Request):
    try:
        body = await request.json()
        method = body.get("method", "")
        params = body.get("params", {})
        request_id = body.get("id", 1)

        if method == "initialize":
            return JSONResponse(
                {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {
                        "protocolVersion": "2026-01-14",
                        "capabilities": {"tools": {}},
                        "serverInfo": {"name": "SemiProcess MCP", "version": "2.0.0"},
                    },
                }
            )
        if method == "notifications/initialized":
            return JSONResponse({"jsonrpc": "2.0", "id": request_id, "result": {}})
        if method == "tools/list":
            return JSONResponse({"jsonrpc": "2.0", "id": request_id, "result": {"tools": TOOLS}})
        if method == "tools/call":
            tool_name = params.get("name", "")
            arguments = params.get("arguments", {})
            handler = TOOL_HANDLERS.get(tool_name)
            if not handler:
                return JSONResponse(
                    {"jsonrpc": "2.0", "id": request_id, "error": {"code": -32601, "message": f"Unknown tool: {tool_name}"}}
                )
            try:
                result = handler(**arguments)
                return JSONResponse(
                    {"jsonrpc": "2.0", "id": request_id, "result": {"content": [{"type": "text", "text": result}]}}
                )
            except TypeError as e:
                return JSONResponse(
                    {"jsonrpc": "2.0", "id": request_id, "error": {"code": -32602, "message": f"Invalid parameters: {e}"}}
                )
            except Exception as e:
                return JSONResponse(
                    {"jsonrpc": "2.0", "id": request_id, "error": {"code": -32603, "message": f"Tool execution error: {e}"}}
                )
        return JSONResponse({"jsonrpc": "2.0", "id": request_id, "error": {"code": -32601, "message": f"Method not found: {method}"}})
    except Exception as e:
        return JSONResponse({"jsonrpc": "2.0", "id": 1, "error": {"code": -32700, "message": f"Parse error: {e}"}}, status_code=400)


@app.get("/favicon.ico")
async def favicon():
    return Response(status_code=204)
